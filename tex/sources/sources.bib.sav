% Encoding: UTF-8

@Electronic{backprop,
  author       = {Michael Nielsen},
  month        = dec,
  year         = {2019},
  howpublished = {\url{http://neuralnetworksanddeeplearning.com/chap2.html}},
}

@Article{Menzel2016,
  author    = {C. Menzel and J. Sperrhake and T. Pertsch},
  title     = {Efficient treatment of stacked metasurfaces for optimizing and enhancing the range of accessible optical functionalities},
  journal   = {Physical Review A},
  year      = {2016},
  volume    = {93},
  number    = {6},
  month     = {jun},
  doi       = {10.1103/physreva.93.063832},
  publisher = {American Physical Society ({APS})},
}

@Electronic{convnet_guide,
  author       = {Sumit Saha},
  month        = dec,
  year         = {2018},
  title        = {A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way},
  howpublished = {\url{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}},
}

@InProceedings{Shepard1968,
  author    = {Donald Shepard},
  title     = {A two-dimensional interpolation function for irregularly-spaced data},
  booktitle = {Proceedings of the 1968 23rd {ACM} national conference on -},
  year      = {1968},
  publisher = {{ACM} Press},
  doi       = {10.1145/800186.810616},
}

@Article{Nelder1965,
  author    = {J. A. Nelder, R. Mead},
  title     = {A Simplex Method for Function Minimization},
  journal   = {The Computer Journal},
  year      = {1965},
  volume    = {8},
  number    = {1},
  pages     = {27--27},
  month     = {apr},
  doi       = {10.1093/comjnl/8.1.27},
  publisher = {Oxford University Press ({OUP})},
}

@Article{Srivastava2014,
  author     = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title      = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal    = {J. Mach. Learn. Res.},
  year       = {2014},
  volume     = {15},
  number     = {1},
  pages      = {1929–1958},
  month      = {jan},
  issn       = {1532-4435},
  issue_date = {January 2014},
  keywords   = {regularization, deep learning, neural networks, model combination},
  numpages   = {30},
  publisher  = {JMLR.org},
}

@Article{Redheffer1960,
  author    = {R. M. Redheffer},
  title     = {On a Certain Linear Fractional Transformation},
  journal   = {Journal of Mathematics and Physics},
  year      = {1960},
  volume    = {39},
  number    = {1-4},
  pages     = {269--286},
  month     = {apr},
  doi       = {10.1002/sapm1960391269},
  publisher = {Wiley},
}

@Article{Simovski2007,
  author    = {Constantin R. Simovski and Sergei A. Tretyakov},
  title     = {Local constitutive parameters of metamaterials from an effective-medium perspective},
  journal   = {Physical Review B},
  year      = {2007},
  volume    = {75},
  number    = {19},
  month     = {may},
  doi       = {10.1103/physrevb.75.195111},
  publisher = {American Physical Society ({APS})},
}

@Article{Shelby2001,
  author    = {R. A. Shelby},
  title     = {Experimental Verification of a Negative Index of Refraction},
  journal   = {Science},
  year      = {2001},
  volume    = {292},
  number    = {5514},
  pages     = {77--79},
  month     = {apr},
  doi       = {10.1126/science.1058847},
  publisher = {American Association for the Advancement of Science ({AAAS})},
}

@Article{Yu2014,
  author    = {Nanfang Yu and Federico Capasso},
  title     = {Flat optics with designer metasurfaces},
  journal   = {Nature Materials},
  year      = {2014},
  volume    = {13},
  number    = {2},
  pages     = {139--150},
  month     = {jan},
  doi       = {10.1038/nmat3839},
  publisher = {Springer Science and Business Media {LLC}},
}

@Article{Mueller2017,
  author    = {J.{\hspace{0.167em}}P. Balthasar Mueller and Noah A. Rubin and Robert C. Devlin and Benedikt Groever and Federico Capasso},
  title     = {Metasurface Polarization Optics: Independent Phase Control of Arbitrary Orthogonal States of Polarization},
  journal   = {Physical Review Letters},
  year      = {2017},
  volume    = {118},
  number    = {11},
  month     = {mar},
  doi       = {10.1103/physrevlett.118.113901},
  publisher = {American Physical Society ({APS})},
}

@Article{Noponen1994,
  author    = {Eero Noponen and Jari Turunen},
  title     = {Eigenmode method for electromagnetic synthesis of diffractive elements with three-dimensional profiles},
  journal   = {Journal of the Optical Society of America A},
  year      = {1994},
  volume    = {11},
  number    = {9},
  pages     = {2494},
  month     = {sep},
  doi       = {10.1364/josaa.11.002494},
  publisher = {The Optical Society},
}

@Article{Liu2018,
  author    = {Dianjing Liu and Yixuan Tan and Erfan Khoram and Zongfu Yu},
  title     = {Training Deep Neural Networks for the Inverse Design of Nanophotonic Structures},
  journal   = {{ACS} Photonics},
  year      = {2018},
  volume    = {5},
  number    = {4},
  pages     = {1365--1369},
  month     = {feb},
  doi       = {10.1021/acsphotonics.7b01377},
  publisher = {American Chemical Society ({ACS})},
}

@Misc{FOMO,
  author       = {Thomas Pertsch},
  title        = {Fundermentals of Modern Optics},
  howpublished = {Lecture Script},
  month        = nov,
  year         = {2016},
}

@Book{Maier2007,
  title     = {Plasmonics},
  publisher = {Springer-Verlag GmbH},
  year      = {2007},
  author    = {Maier, Stefan Alexander},
  isbn      = {0387331506},
  date      = {2007-05-15},
  ean       = {9780387331508},
  url       = {https://www.ebook.de/de/product/5739694/stefan_alexander_maier_plasmonics.html},
}

@Article{Zhao2013,
  author    = {Yang Zhao and Jinwei Shi and Liuyang Sun and Xiaoqin Li and Andrea Al{\`{u}}},
  title     = {Alignment-Free Three-Dimensional Optical Metamaterials},
  journal   = {Advanced Materials},
  year      = {2013},
  volume    = {26},
  number    = {9},
  pages     = {1439--1445},
  month     = {dec},
  doi       = {10.1002/adma.201304379},
  publisher = {Wiley},
}

@Article{Dequaire2016,
  author      = {Julie Dequaire and Dushyant Rao and Peter Ondruska and Dominic Wang and Ingmar Posner},
  title       = {Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks},
  abstract    = {This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach [Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.},
  date        = {2016-09-29},
  eprint      = {http://arxiv.org/abs/1609.09365v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1609.09365v1:PDF},
  keywords    = {cs.CV, cs.AI, cs.LG, cs.RO},
}

@Misc{youtube,
  author       = {Paul Covington, Jay Adams, Emre Sargin},
  title        = {Deep Neural Networks for YouTube Recommendations},
  howpublished = {Google Research},
}

@Article{Karras2018,
  author      = {Tero Karras and Samuli Laine and Timo Aila},
  title       = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  abstract    = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  date        = {2018-12-12},
  eprint      = {http://arxiv.org/abs/1812.04948v3},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1812.04948v3:PDF},
  keywords    = {cs.NE, cs.LG, stat.ML},
}

@Article{Ioffe2015,
  author      = {Sergey Ioffe and Christian Szegedy},
  title       = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  abstract    = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.},
  date        = {2015-02-11},
  eprint      = {http://arxiv.org/abs/1502.03167v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1502.03167v3:PDF},
  keywords    = {cs.LG},
}

@Article{Ma2018,
  author    = {Wei Ma and Feng Cheng and Yongmin Liu},
  title     = {Deep-Learning-Enabled On-Demand Design of Chiral Metamaterials},
  journal   = {{ACS} Nano},
  year      = {2018},
  volume    = {12},
  number    = {6},
  pages     = {6326--6334},
  month     = {jun},
  doi       = {10.1021/acsnano.8b03569},
  publisher = {American Chemical Society ({ACS})},
}

@InProceedings{Fan2019,
  author    = {J. A. {Fan}},
  title     = {Generating high performance, topologically-complex metasurfaces with neural networks},
  booktitle = {2019 Conference on Lasers and Electro-Optics (CLEO)},
  year      = {2019},
  pages     = {1-2},
}

@Article{TransposedConv,
  author      = {Vincent Dumoulin and Francesco Visin},
  title       = {A guide to convolution arithmetic for deep learning},
  abstract    = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  date        = {2016-03-23},
  eprint      = {http://arxiv.org/abs/1603.07285v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1603.07285v2:PDF},
  keywords    = {stat.ML, cs.LG, cs.NE},
}

@Article{RandomSearch,
  author  = {James Bergstra and Yoshua Bengio},
  title   = {Random Search for Hyper-Parameter Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {10},
  pages   = {281-305},
  url     = {http://jmlr.org/papers/v13/bergstra12a.html},
}

@Electronic{fill_in_detailsq,
  author       = {Jason Brownlee},
  month        = jun,
  year         = {2019},
  title        = {How to use the UpSampling2D and Conv2DTranspose Layers in Keras},
  howpublished = {https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/},
}

@Misc{cat,
  author       = {Arthur Douillard, Yifu Chen, Matthieu Cord},
  title        = {Reeseaux convolutionnels pour l’image},
  howpublished = {\url{https://arthurdouillard.com/files/rdfia_resources/tp6-7.pdf}},
  month        = nov,
  year         = {2},
}

@Comment{jabref-meta: databaseType:bibtex;}
