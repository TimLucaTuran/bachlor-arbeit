\begin{tabular}{ll}
    \toprule
    Input: &
    \begin{tabular}[t]{@{}l@{}}
        Spectrum $I=(I_\s{x}, \,I_\s{y})$ a $\lambda \times 2$ array\\
        $I_\s{x/y}$...X- and Y-transmission spectra,
        $\lambda$...number of wavelengths
    \end{tabular}\\
    Output: &
    \begin{tabular}[t]{@{}l@{}l@{}}
    two sets layer parameters $p=(w, \, l, \, t, \, \Lambda), \, m, \, g$ and stack parameters
    $\varphi, \, h$ \\
    $w$...width, $l$...length, $t$...thickness, $\Lambda$...Period,
    $m$...material, $g$...geometry \\
    $\varphi$...rotation angle, $h$...distance between layers
    \end{tabular}\\
    \bottomrule
\end{tabular}

\paragraph{Network Architecture}
This module is a 1D Convolutional Neural Network instead of the simple Multi Layer Perceptron. It was chosen to utilize the translational invariance of ConvNets. For example the concept "peak" should be learned independent of its position in the spectrum. As described in section \ref{sec:NN_bg} a ConvNet provides this functionality. Another constraint on the network architecture arises from the different kind of outputs.
$p = (w, \, l, \, t, \, \Lambda), \, \varphi$ and $h$
are continuous and $m, \, g$ are discrete/categorical.
These need different activation functions $\sigma$ to reach the different value ranges. The continuous outputs are mostly bounded by physical constraints and $m, \, g \in [0, \, 1]$ as they are \textit{one hot encoded} meaning $1 \rightarrow$ "The layer has this property" and
$0 \rightarrow$ "The layer does not have this property".


The different outputs also need different cost functions $C(y, y')$ during training where $y'$ is the networks output and $y$ is the known solution. For the continuous output one can simply use the mean squared error

\begin{equation}
    C_\s{mse}(\vb y, \, \vb y') = \sum_i \qty(y_i - y_i')^2
\end{equation}

\noindent
as all outputs are equally important and the cost function should be indifferent on whether the networks prediction is over or under target. For the categorical output the network learns quicker with the \textit{Categorical Cross-Entropy} error.

\begin{equation}
    C_\s{ce}(\vb y, \, \vb y') = - \sum_i y_i \log y'_i,
\end{equation}

\noindent
Using this error if the network predicts $y'_i = 0$ for all categories then $C_\s{ce} \rightarrow \infty$. \note{why is this better?}

The final architecture is similar to the example given in figure \ref{fig:bg:NN_example} while meeting the above-mentioned constraints:

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{al_NN_architecture}
    \caption{The network starts with 4 pairs of convolutional and pooling layers. The convolutions are characterized by (\textit{number of kernels}, \textit{kernel size}). The kernel size is always 5 and the number of kernels is gradually increased. Then the Network splits into a discrete and a continuous branch via two Dense layers with (\textit{number of neurons}). In the discrete branch a dropout is applied to the dense layer where (0.5) is (\textit{fraction of neurons to drop}).
    All the internal activations $\sigma_\s{r}$ are ReLu's and the final activations $\sigma_\s{s}$ and $\sigma_{l}$ are a sigmoid and a linear function.}
    \label{fig:al:NN_architecture}
\end{figure}

\newpage
\paragraph{Network Training}
To train a Neural Network one needs a training set $(X, \, Y)$ of known input output pairs. In this case they are generated using the pre simulated single layers in the database which are randomly combined into a stacks. Then this stacks X- and Y-transmission spectra $(I_x, \, I_y)$ are calculated via SASA.
That means $(I_x, \, I_y) \in X$ are the networks input and the random parameters $(p_1, \, m_1, \, g_1, \, p_2, \, m_2, \, g_2) \in Y$ are the output.
Using this approach the following accuracy is reached:

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\linewidth]{al_random_plot}
    \caption{\note{wrong plot} Shown are the total loss and the training and validation accuracy for both the continuous and the discrete branch. After each epoch the network is validated on data not used in training to check for overfitting.}
    \label{}
\end{figure}

Training and validation accuracy are very similar which indicates that there is no overfitting. The discrete accuracy quickly reaches a maximum of $\sim 70\%$
which is less than expected for this classification problem. The issue here lies not in the networks arichtecture but in the data generation process. In \note{missing}
we have shown that for a two layer stack the transmission spectrum is the same for both directions. That means the data generation can result in two different stacks which share the same spectrum. Lets consider a stack where one layer is Aluminium and the other is Gold. As both of them produce the same spectrum one time the network is taught that the first layer is Gold and another time its taught the complete opposite. Actually if the network is trained this way it only ever predicts stacks with layers of equal materials because this is the only constellation it can get right.
\\

\begin{figure}[H]
    \floatbox[{\capbeside\thisfloatsetup{capbesideposition={right,top},capbesidewidth=8cm}}]{figure}[\FBwidth]
    {\caption{A stack of one Gold and one Aluminium layer separated by a glass spacer. Both orientations produce the same spectrum which leads to issues when using completely random stacks to train the network.}
    \label{fig:test}}
    {\includegraphics[width=.4\textwidth]{al_flipped_stack}}
\end{figure}
