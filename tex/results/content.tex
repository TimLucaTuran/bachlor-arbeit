\section{Conclusion} \label{sec:conclusion}

\subsection{Summary}
We started off by discussing the physics behind the chosen optical system of stacked plasmonic metasurfaces. 
We learned how Maxwell's equations govern the behavior of light at the interfaces between layers and how to use the $S$-matrix calculus and SASA to describe stacks as a whole. This calculus came with a number of boundary conditions the algorithm needed to obey. Then, we turned to neural networks and discussed how they are build and trained. We settled on Convolutional Neural Networks as they would be able to efficiently use the spatial information in a transmission spectrum.
\\

\indent
The first step in implementing the algorithm was training a network on spectra generated by SASA. Here we encountered the may-to-one problem for the first time, where multiple designs mapped to a single spectrum. We had already derived that, under certain conditions, metasurface stacks would produce the same spectrum in the top-to-bottom and bottom-to-top orientations. After removing these equivalent stacks the network could be trained successfully. It was very accurate in finding the discrete design parameters to a spectrum but the predictions for the continuous parameters were only good enough as an initial guess.
This is why the another optimization step was added. A conventional simplex further tunes the continuous parameters and at every step the performance of the new stack is evaluated via SASA. This optimization loop is repeated until the target accuracy is reached.


\subsection{Evaluation} \label{sec:eval}
The algorithm is generally able to reproduce spectra from stacks generated based on the database. 
This is a good first test because we know these spectra are possible and if the algorithm could not reproduce them something would be wrong. However, the goal was not to reproduce known spectra but being able to find design parameters to new ones. Here the evaluation has to be more nuanced. Yes, some new transmission spectra, like Gau√ü peaks, dips and sinus functions, are possible but the first limitation becomes apparent.
The maximum transmission has to stay below $\sim 50\%$. This can be understood by the choice in materials. Plasmonic metasurfaces will always dissipate energy from the field via ohmic losses. Another limitation can be seen in figure . Even when the target function is approximated successfully there might be additional unwanted features in the spectrum. 
\\

\indent
Almost comically, some targets that would be trivial for a human are very challenging for the algorithm. If a human was tasked to create the maximum possible transmission at all wavelengths, he or she would probably use zero thickness metasurfaces or metasurfaces which only consist of holes. When the algorithm is given a transmission spectrum that is one everywhere it gets confused to a point of predicting negative parameters.
They are chosen despite being completely out of bounds and thus very negatively affecting the simplexes loss function through equation \eqref{eq:al:simlex_loss}.
However, even this behavior can be explained because there are no zero thickness or hole only metasurfaces in the database we cannot expect the simplex to know about them.
Additionally, the less reachable a task is, the higher is the total loss and the less important is the boundary term explaining the nonsensical negative parameters. This leads over to the next section of what could be improved about the algorithm.
\newpage

\subsection{Outlook}\label{sec:outlook}
In section \ref{sec:algorithm} we have seen that it is very difficult or even impossible to solve the inverse problem $I \rightarrow \mc D$ directly with a neural network because of multiple designs mapping to a single spectrum.
For the algorithm, we solved this by adding a conventional optimization method after the network to tune the parameters the network was not able to set correctly.
This had some success as described in \ref{sec:eval}  but this approach is also a concession because we loose many of the advantages neural networks bring. Conventional optimization methods are more computationally expensive than a single forward pass in a neural network.
More importantly, a simplex combined with interpolation is not able to generalize beyond the database.
Having one network being able to solve $I \rightarrow \mc D$ directly would be the best solution.
\\

\indent As this is a well known problem numerous solutions have been proposed. A very interesting route was taken by Liu et al. \cite{Liu2018}. They approached the inverse problem by first solving the forward problem in our case $\mc D \rightarrow I$ and then building a combined network in a kind of tandem structure 
$I \rightarrow \mc D \rightarrow I'$
where the cost function is 
$C = C \qty(I, \, I')$.
This means during training the network is given a target spectrum $I$ and predicts the corresponding design parameters $\mc D$. This design is then fed into the forward model and produces a spectrum $I'$ and the cost function depends on the difference of $I$ and $I'$. This approach completely circumvents the issue of multiple designs mapping to a single spectrum because it only depends on the differences between the target and the produced spectrum. This approach was tried but could ultimately not be implemented successfully due to overfitting issues in the combined model as described in appendix \ref{sec:apdx_A}. However, it should be possible with the right combination of network architecture, regularization and hardware.
\\    

\indent
The larger a project becomes, the more \textit{hyperparmeters} have to be set. Hyperparmeters are a term from machine learning and classically include parameters such as the \hyperref[hyp:learnigrate]{learning rate} or the drop rate in Dropout layers. We could also include the network architecture, that is how many layers or how many neurons per layer the network should have. One could even consider the question, of how many different materials to use and how densely to simulate the parameters space to each material, a hyperparameter.
At some point it is not enough to set these parameters based on trial and error and intuition and one has to start to use more thorough methods. There are many options including cross validation \note{cite} and ... \note{cite}.
\\

\indent
Lastly, the initial numerical simulation of metasurfaces were done on a compute cluster but all other computations were done on the CPU of a reasonably modern laptop. That includes the training of neural networks and the simplex optimization. However, due to the algebraic nature of the calculations when training a neural network these are best done on at least one GPU. Better Hardware could improve performance by enabling the usage of bigger models and more training data. 
\\

\indent

