% !TeX root = ../bachlor-arbeit.tex
\section{Conclusion} \label{sec:conclusion}
To summarize, we started off by discussing the physics behind the chosen optical system of stacked plasmonic metasurfaces. 
We learned how Maxwell's equations govern the behavior of light at the interfaces between layers and how to use the $S$-matrix calculus and the semi analytic stacking algorithm to describe stacks as a whole. This calculus came with a number of boundary conditions the algorithm needed to obey, such as the minium distance between layers and the maximum period of metaatoms. Then, we turned to neural networks and discussed how they are build and trained. We settled on Convolutional Neural Networks as they would be able to efficiently use the spatial information in a transmission spectrum.
\\

\indent
The first step in implementing the algorithm was training a network on spectra generated by SASA. Here we encountered the many-to-one problem for the first time, where multiple designs mapped to a single spectrum. For instance, certain metasurface stacks would produce the same spectrum independent of the layer order. This resulted in multiple designs mapping to a single spectrum and impacted network training negatively. After removing these equivalent stacks the network could be trained successfully. It was very accurate in finding the discrete design parameters to a spectrum but the predictions for the continuous parameters were only good enough as an initial guess.
This is why the another optimization step was added. A conventional simplex further tunes the continuous parameters and at every step the performance of the new stack is evaluated with the semi analytic stacking algorithm. 
\\

\indent
In the second part of these conclusions, some possibilities are shown how the algorithm could be improved.
In section \ref{sec:algorithm} we have seen that it is very difficult or even impossible to solve the inverse problem (spectrum $I$ $\rightarrow$ design $\mc D$) directly with a neural network because of multiple designs mapping to a single spectrum.
For the algorithm, we solved this by adding a conventional optimization method after the network to tune the parameters the network was not able to set correctly.
This had some success as described in \ref{sec:eval}  but this approach is also a concession because we loose many of the advantages neural networks bring. Conventional optimization methods are more computationally expensive than a single forward pass in a neural network.
More importantly, a simplex combined with interpolation is not able to generalize beyond the database.
Having one network being able to solve $I \rightarrow \mc D$ directly would be the best solution.
\\

\indent
As this is a well known problem numerous solutions have been proposed. A very interesting route was taken by Liu et al. \cite{Liu2018}. They approached the inverse problem by first solving the forward problem in our case $\mc D \rightarrow I$ and then building a combined network in a kind of tandem structure 
$I \rightarrow \mc D \rightarrow I'$
where the cost function is 
$C = C \qty(I, \, I')$.
This means during training the network is given a target spectrum $I$ and predicts the corresponding design parameters $\mc D$. This design is then fed into the forward model and produces a spectrum $I'$ and the cost function depends on the difference of $I$ and $I'$. This approach completely circumvents the issue of multiple designs mapping to a single spectrum because it only depends on the differences between the target and the produced spectrum. This approach was tried but could ultimately not be implemented successfully due to overfitting issues in the combined model as described in appendix \ref{sec:apdx_A}. However, it should be possible with the right combination of network architecture, regularization and hardware.
\\    

\indent
The larger a project becomes, the more \textit{hyperparmeters} have to be set. Hyperparmeters are a term from machine learning and traditionally include parameters such as the \hyperref[hyp:learnigrate]{learning rate} or the \hyperref[hyp:dropout]{drop rate} in Dropout layers. We could also include the network architecture, that is how many layers or how many neurons per layer the network should have. One could even include the question, of how many different materials to use and how densely to simulate the parameters space to each material.
At some point it is not enough to set these parameters based on trial and error and intuition. One has to start to use more thorough methods. There are many options including \textit{Grid Search} or \textit{Random Search} \cite{RandomSearch} that could be tried.
\\
\\

\indent
Lastly, the initial numerical simulation of metasurfaces were done on a compute cluster but all other computations were done on the CPU of a reasonably modern laptop. That includes the training of neural networks and the simplex optimization. However, due to the algebraic nature of the calculations when training a neural network, these are best done on at least one GPU. Better Hardware could improve performance by enabling the usage of bigger models and more training data. 

